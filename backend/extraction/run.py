"""
æŠ½å–æ¨¡å— CLI å…¥å£

ç”¨æ³•:
    cd backend
    
    # ä»æ–‡ä»¶æŠ½å–ä¸–ç•Œä¹¦
    python -m extraction.run worldinfo input.txt -o output.json
    
    # ä»æ–‡ä»¶æŠ½å–ï¼ˆé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—ï¼‰
    python -m extraction.run worldinfo novel.txt -o worldinfo.json --chunk-size 6000
    
    # æŒ‡å®šæ¨¡å‹
    python -m extraction.run worldinfo input.txt --model gpt-4o
    
    # ç›´æ¥å¯¼å…¥åˆ° content.db
    python -m extraction.run worldinfo input.txt --import-db
"""
import argparse
import json
import sys
import math
import hashlib
from pathlib import Path

# ç¡®ä¿ backend ç›®å½•åœ¨ Python è·¯å¾„ä¸­
backend_dir = Path(__file__).parent.parent
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))


def cmd_worldinfo(args):
    """ä¸–ç•Œä¹¦æŠ½å–å‘½ä»¤"""
    from extraction import WorldInfoExtractor
    from extraction.base import ExtractionResult
    from extraction.config import get_extraction_config
    
    config = get_extraction_config()

    # æ£€æŸ¥è¾“å…¥å‚æ•°
    if not args.input and not args.input_dir:
        if config.input_dir:
            args.input_dir = config.input_dir
        elif config.input:
            args.input = config.input
        else:
            print("âŒ è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶æˆ–ä½¿ç”¨ --input-dir æŒ‡å®šç›®å½•")
            return 1
    
    input_display = args.input_dir if args.input_dir else args.input
    print(f"ğŸ“– ä¸–ç•Œä¹¦æŠ½å–")
    print(f"   è¾“å…¥: {input_display}")
    
    # åˆ›å»ºæŠ½å–å™¨
    extractor_kwargs = {}
    if args.model:
        extractor_kwargs["model"] = args.model
    if args.temperature:
        extractor_kwargs["temperature"] = args.temperature
    
    llm_merge_enabled = bool(args.llm_merge)
    if args.prompts_dir:
        extractor_kwargs["prompts_dir"] = args.prompts_dir
    extractor = WorldInfoExtractor(
        enable_llm_merge=llm_merge_enabled,
        **extractor_kwargs
    )

    def _append_jsonl(path: Path, payload: dict) -> None:
        path.parent.mkdir(parents=True, exist_ok=True)
        with path.open("a", encoding="utf-8") as f:
            f.write(json.dumps(payload, ensure_ascii=False) + "\n")

    def _load_processed_chunks(jsonl_path: Path) -> dict[str, dict]:
        """
        åŠ è½½å·²å¤„ç†çš„ chunk è®°å½•
        è¿”å›: {source: {chunk_index: {"hash": chunk_hash, "entries": [...]}}}
        """
        processed = {}
        if not jsonl_path.exists():
            return processed
        try:
            with jsonl_path.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        source = str(Path(obj.get("source", "")))
                        chunk_index = obj.get("chunk_index")
                        chunk_hash = obj.get("chunk_hash")
                        entries = obj.get("entries", [])
                        success = obj.get("success", True)
                        if source and chunk_index is not None and chunk_hash:
                            if source not in processed:
                                processed[source] = {}
                            processed[source][chunk_index] = {
                                "hash": chunk_hash,
                                "entries": entries,
                                "success": bool(success),
                            }
                    except json.JSONDecodeError:
                        continue
        except Exception:
            return processed
        return processed

    def _file_hash(text: str) -> str:
        return hashlib.sha256(text.encode("utf-8")).hexdigest()

    def _estimate_tokens(text: str) -> int:
        """
        ç²—ç•¥ token ä¼°ç®—ï¼š
        - CJK å­—ç¬¦æŒ‰ 1 token
        - é CJK æŒ‰ 4 å­—ç¬¦ ~ 1 token
        """
        if not text:
            return 0
        cjk = 0
        for ch in text:
            code = ord(ch)
            if (
                0x4E00 <= code <= 0x9FFF  # CJK Unified Ideographs
                or 0x3400 <= code <= 0x4DBF  # CJK Extension A
                or 0x20000 <= code <= 0x2A6DF  # CJK Extension B
                or 0x2A700 <= code <= 0x2B73F  # CJK Extension C
                or 0x2B740 <= code <= 0x2B81F  # CJK Extension D
                or 0x2B820 <= code <= 0x2CEAF  # CJK Extension E
                or 0xF900 <= code <= 0xFAFF  # CJK Compatibility Ideographs
            ):
                cjk += 1
        other = len(text) - cjk
        return cjk + math.ceil(other / 4)

    def _estimate_messages_tokens(messages: list[dict]) -> int:
        return sum(_estimate_tokens(m.get("content", "")) for m in messages)

    def _estimate_file_tokens(text: str) -> dict:
        # æ„å»ºä¸»æç¤ºè¯ï¼ˆä¸å«æ¨¡å‹è¾“å‡ºï¼‰
        first_messages = extractor.build_prompt(text)
        first_tokens = _estimate_messages_tokens(first_messages)
        total_calls = 1
        total_tokens = first_tokens

        # Gleaning é¢å¤–è°ƒç”¨ï¼ˆä¸è®¡å…¥ assistant è¾“å‡ºé•¿åº¦ï¼Œä½œä¸ºä¿å®ˆä¼°è®¡ï¼‰
        if extractor.enable_gleaning:
            gleaning_messages = [
                {"role": "system", "content": extractor.system_prompt},
                {"role": "user", "content": extractor.user_prompt_template.format(text=text)},
                {"role": "assistant", "content": ""},  # ä¼°ç®—ä¸åŒ…å«é¦–æ¬¡è¾“å‡º
                {"role": "user", "content": extractor.gleaning_prompt_template.format(text=text)},
            ]
            total_calls += 1
            total_tokens += _estimate_messages_tokens(gleaning_messages)

        return {
            "calls": total_calls,
            "tokens": total_tokens,
        }

    def _estimate_chunks_tokens(text: str) -> dict:
        chunks = extractor._split_text(
            text,
            args.chunk_size,
            args.overlap or 500,
            strategy=args.chunk_strategy,
            chapter_max_chars=args.chapter_max,
        )
        total_calls = 0
        total_tokens = 0
        for chunk in chunks:
            info = _estimate_file_tokens(chunk)
            total_calls += info["calls"]
            total_tokens += info["tokens"]
        return {
            "chunks": len(chunks),
            "calls": total_calls,
            "tokens": total_tokens,
        }
    
    # è¯»å–æ–‡ä»¶æˆ–ç›®å½•
    input_path = Path(args.input_dir) if args.input_dir else Path(args.input)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨: {input_path}")
        return 1

    # ç›®å½•æ¨¡å¼ï¼šè¯»å–è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰ .mdï¼ˆæŒ‰æ–‡ä»¶åé¡ºåºï¼‰
    if input_path.is_dir():
        pattern = "**/*.md" if args.recursive else "*.md"
        md_files = sorted(input_path.glob(pattern))
        if not md_files:
            print(f"âŒ ç›®å½•å†…æœªæ‰¾åˆ° .md æ–‡ä»¶: {input_path}")
            return 1
        print(f"   æ–‡ä»¶æ•°: {len(md_files)}")

        # è‡ªåŠ¨è®¾ç½®å¢é‡æ–‡ä»¶è·¯å¾„ï¼ˆæ”¾åœ¨ input ç›®å½•å†…ï¼‰
        if not args.output_jsonl:
            args.output_jsonl = str(input_path / ".worldinfo.partial.jsonl")
        print(f"   å¢é‡æ–‡ä»¶: {args.output_jsonl}")

        # è‡ªåŠ¨å¯ç”¨æ–­ç‚¹ç»­è·‘ï¼ˆchunk çº§åˆ«ï¼‰
        processed_chunks = _load_processed_chunks(Path(args.output_jsonl))
        if processed_chunks:
            total_chunks_done = sum(len(v) for v in processed_chunks.values())
            print(f"   å·²æ£€æµ‹åˆ°å·²å¤„ç†: {len(processed_chunks)} ä¸ªæ–‡ä»¶, {total_chunks_done} ä¸ª chunk")

        if args.estimate_tokens:
            total_calls = 0
            total_tokens = 0
            total_files = len(md_files)
            for idx, md_path in enumerate(md_files, start=1):
                text = md_path.read_text(encoding="utf-8")
                file_hash = _file_hash(text)
                if processed_sources.get(str(md_path)) == file_hash:
                    percent = int(idx / total_files * 100)
                    print(f"   ä¼°ç®—: [{idx}/{total_files} {percent}%] {md_path.name} -> å·²å¤„ç†ï¼Œè·³è¿‡")
                    continue
                if args.chunk_strategy != "fixed" or (args.chunk_size and len(text) > args.chunk_size):
                    info = _estimate_chunks_tokens(text)
                    total_calls += info["calls"]
                    total_tokens += info["tokens"]
                    percent = int(idx / total_files * 100)
                    print(
                        f"   ä¼°ç®—: [{idx}/{total_files} {percent}%] {md_path.name} -> chunks={info['chunks']}, callsâ‰ˆ{info['calls']}, tokensâ‰ˆ{info['tokens']}"
                    )
                else:
                    info = _estimate_file_tokens(text)
                    total_calls += info["calls"]
                    total_tokens += info["tokens"]
                    percent = int(idx / total_files * 100)
                    print(
                        f"   ä¼°ç®—: [{idx}/{total_files} {percent}%] {md_path.name} -> callsâ‰ˆ{info['calls']}, tokensâ‰ˆ{info['tokens']}"
                    )
            if args.llm_merge:
                print("   ä¼°ç®—æç¤º: --llm-merge çš„åˆå¹¶è°ƒç”¨ä¸åœ¨æ­¤ä¼°ç®—å†…ï¼ˆå–å†³äºæŠ½å–åæ¡ç›®é•¿åº¦ï¼‰ã€‚")
            print(f"   ä¼°ç®—æ±‡æ€»: callsâ‰ˆ{total_calls}, tokensâ‰ˆ{total_tokens}")
            print("   ä¼°ç®—è¯´æ˜: ä¸åŒ…å«æ¨¡å‹è¾“å‡ºé•¿åº¦ï¼Œä»…ä¸ºè¾“å…¥æç¤ºè¯çš„ç²—ç•¥ä¼°ç®—ã€‚")
            if args.estimate_only:
                return 0

        results_all = []
        any_failed = False
        total_files = len(md_files)
        for idx, md_path in enumerate(md_files, start=1):
            text = md_path.read_text(encoding="utf-8")
            percent = int(idx / total_files * 100)
            print(f"   å¤„ç†: [{idx}/{total_files} {percent}%] {md_path.name} ({len(text)} å­—ç¬¦)")

            # åˆ†å—å¤„ç†ï¼ˆå¸¦ chunk çº§åˆ«æ–­ç‚¹ç»­è·‘ï¼‰
            chunks = extractor._split_text(
                text,
                args.chunk_size,
                args.overlap or 500,
                strategy=args.chunk_strategy,
                chapter_max_chars=args.chapter_max,
            )
            chunk_count = len(chunks)
            print(
                f"   åˆ†å—å¤„ç†: ç­–ç•¥={args.chunk_strategy}, chunk_size={args.chunk_size}, chapter_max={args.chapter_max}ï¼Œå…± {chunk_count} å—"
            )

            file_key = str(md_path)
            file_processed_chunks = processed_chunks.get(file_key, {})
            chunk_results = []
            skipped = 0
            success = 0

            file_failed = False
            for ci, chunk in enumerate(chunks):
                chunk_hash = _file_hash(chunk)
                cached = file_processed_chunks.get(ci)
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆå“ˆå¸Œä¸€è‡´ï¼‰
                if cached and cached.get("hash") == chunk_hash and cached.get("success"):
                    # ä»ç¼“å­˜æ¢å¤
                    chunk_results.append(ExtractionResult(
                        success=True,
                        data=cached.get("entries", []),
                        source=f"{md_path}#chunk{ci}",
                    ))
                    skipped += 1
                    print(f"     chunk [{ci+1}/{chunk_count}] -> å·²å¤„ç†ï¼Œè·³è¿‡")
                    continue

                # è°ƒç”¨ LLM æŠ½å–
                max_retries = max(1, int(args.retry_max))
                result = None
                attempts_used = 0
                for attempt in range(1, max_retries + 1):
                    attempts_used = attempt
                    print(f"     chunk [{ci+1}/{chunk_count}] æŠ½å–ä¸­... (å°è¯• {attempt}/{max_retries})")
                    result = extractor.extract(chunk)
                    if result.success:
                        break
                    print(f"     chunk [{ci+1}/{chunk_count}] æŠ½å–å¤±è´¥: {result.error}")
                
                if result.success:
                    success += 1
                    chunk_results.append(result)
                    # ç«‹å³å†™å…¥ jsonlï¼ˆchunk çº§åˆ«ï¼‰
                    _append_jsonl(
                        Path(args.output_jsonl),
                        {
                            "source": file_key,
                            "chunk_index": ci,
                            "chunk_hash": chunk_hash,
                            "entries": result.data,
                            "success": True,
                            "attempts": attempts_used,
                        }
                    )
                else:
                    file_failed = True
                    chunk_results.append(result)
                    _append_jsonl(
                        Path(args.output_jsonl),
                        {
                            "source": file_key,
                            "chunk_index": ci,
                            "chunk_hash": chunk_hash,
                            "entries": [],
                            "success": False,
                            "attempts": attempts_used,
                            "error": result.error or "unknown error",
                        }
                    )

            print(f"   åˆ†å—ç»“æœ: {success} æˆåŠŸ, {skipped} è·³è¿‡, {chunk_count - success - skipped} å¤±è´¥")

            # ç›®å½•æ¨¡å¼ä¸‹é¿å…å¯¹æ¯ä¸ªæ–‡ä»¶å•ç‹¬åš LLM åˆå¹¶ï¼Œç•™ç»™å…¨å±€åˆå¹¶å¤„ç†
            if file_failed:
                any_failed = True
                print(f"   âš ï¸ æ–‡ä»¶å­˜åœ¨å¤±è´¥ chunkï¼Œè·³è¿‡è¯¥æ–‡ä»¶åˆå¹¶: {md_path}")
                results_all.append(ExtractionResult(
                    success=False,
                    data=[],
                    source=str(md_path),
                    error="chunk_failed",
                ))
            else:
                print(f"   åˆå¹¶ä¸­: {md_path.name} -> {len(chunk_results)} chunks")
                if args.llm_merge:
                    prev_merge = extractor.enable_llm_merge
                    extractor.enable_llm_merge = False
                    entries = extractor.merge_results(chunk_results)
                    extractor.enable_llm_merge = prev_merge
                else:
                    entries = extractor.merge_results(chunk_results)
                print(f"   åˆå¹¶å®Œæˆ: {md_path.name} -> {len(entries)} æ¡ç›®")

                results_all.append(ExtractionResult(
                    success=True,
                    data=entries,
                    source=str(md_path),
                ))

        # ç›®å½•æ¨¡å¼åˆå¹¶æ‰€æœ‰æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶é¡ºåºï¼‰
        if any_failed:
            print("âŒ å­˜åœ¨å¤±è´¥ chunkï¼Œå·²è·³è¿‡æœ€ç»ˆåˆå¹¶ã€‚è¯·åŸºäº jsonl è¿›è¡Œé‡è¯•åå†åˆå¹¶ã€‚")
            return 2
        print(f"âœ… å¼€å§‹æœ€ç»ˆåˆå¹¶: {len(results_all)} ä¸ªæ–‡ä»¶")
        entries = extractor.merge_results(results_all)
        print(f"âœ… æœ€ç»ˆåˆå¹¶å®Œæˆ: {len(entries)} æ¡ç›®")
    else:
        # æ–‡ä»¶æ¨¡å¼
        text = input_path.read_text(encoding="utf-8")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")

        if args.estimate_tokens:
            if args.chunk_strategy != "fixed" or (args.chunk_size and len(text) > args.chunk_size):
                info = _estimate_chunks_tokens(text)
                print(
                    f"   ä¼°ç®—: chunks={info['chunks']}, callsâ‰ˆ{info['calls']}, tokensâ‰ˆ{info['tokens']}"
                )
            else:
                info = _estimate_file_tokens(text)
                print(
                    f"   ä¼°ç®—: callsâ‰ˆ{info['calls']}, tokensâ‰ˆ{info['tokens']}"
                )
            if args.llm_merge:
                print("   ä¼°ç®—æç¤º: --llm-merge çš„åˆå¹¶è°ƒç”¨ä¸åœ¨æ­¤ä¼°ç®—å†…ï¼ˆå–å†³äºæŠ½å–åæ¡ç›®é•¿åº¦ï¼‰ã€‚")
            print("   ä¼°ç®—è¯´æ˜: ä¸åŒ…å«æ¨¡å‹è¾“å‡ºé•¿åº¦ï¼Œä»…ä¸ºè¾“å…¥æç¤ºè¯çš„ç²—ç•¥ä¼°ç®—ã€‚")
            if args.estimate_only:
                return 0

        # è‡ªåŠ¨è®¾ç½®å¢é‡æ–‡ä»¶è·¯å¾„ï¼ˆæ”¾åœ¨æ–‡ä»¶åŒç›®å½•ï¼‰
        if not args.output_jsonl:
            args.output_jsonl = str(input_path.parent / f".{input_path.stem}.partial.jsonl")
        print(f"   å¢é‡æ–‡ä»¶: {args.output_jsonl}")

        # åŠ è½½å·²å¤„ç†çš„ chunk
        processed_chunks = _load_processed_chunks(Path(args.output_jsonl))
        file_key = str(input_path)
        file_processed_chunks = processed_chunks.get(file_key, {})
        if file_processed_chunks:
            print(f"   å·²æ£€æµ‹åˆ°å·²å¤„ç†: {len(file_processed_chunks)} ä¸ª chunk")

        # æ‰§è¡ŒæŠ½å–ï¼ˆchunk çº§åˆ«æ–­ç‚¹ç»­è·‘ï¼‰
        chunks = extractor._split_text(
            text,
            args.chunk_size,
            args.overlap or 500,
            strategy=args.chunk_strategy,
            chapter_max_chars=args.chapter_max,
        )
        chunk_count = len(chunks)
        print(
            f"   åˆ†å—å¤„ç†: ç­–ç•¥={args.chunk_strategy}, chunk_size={args.chunk_size}, chapter_max={args.chapter_max}ï¼Œå…± {chunk_count} å—"
        )

        chunk_results = []
        skipped = 0
        success = 0

        any_failed = False
        for ci, chunk in enumerate(chunks):
            chunk_hash = _file_hash(chunk)
            cached = file_processed_chunks.get(ci)

            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆå“ˆå¸Œä¸€è‡´ï¼‰
            if cached and cached.get("hash") == chunk_hash and cached.get("success"):
                chunk_results.append(ExtractionResult(
                    success=True,
                    data=cached.get("entries", []),
                    source=f"{input_path}#chunk{ci}",
                ))
                skipped += 1
                print(f"     chunk [{ci+1}/{chunk_count}] -> å·²å¤„ç†ï¼Œè·³è¿‡")
                continue

            # è°ƒç”¨ LLM æŠ½å–
            max_retries = max(1, int(args.retry_max))
            result = None
            attempts_used = 0
            for attempt in range(1, max_retries + 1):
                attempts_used = attempt
                print(f"     chunk [{ci+1}/{chunk_count}] æŠ½å–ä¸­... (å°è¯• {attempt}/{max_retries})")
                result = extractor.extract(chunk)
                if result.success:
                    break
                print(f"     chunk [{ci+1}/{chunk_count}] æŠ½å–å¤±è´¥: {result.error}")

            if result.success:
                success += 1
                chunk_results.append(result)
                # ç«‹å³å†™å…¥ jsonlï¼ˆchunk çº§åˆ«ï¼‰
                _append_jsonl(
                    Path(args.output_jsonl),
                    {
                        "source": file_key,
                        "chunk_index": ci,
                        "chunk_hash": chunk_hash,
                        "entries": result.data,
                        "success": True,
                        "attempts": attempts_used,
                    }
                )
            else:
                any_failed = True
                chunk_results.append(result)
                _append_jsonl(
                    Path(args.output_jsonl),
                    {
                        "source": file_key,
                        "chunk_index": ci,
                        "chunk_hash": chunk_hash,
                        "entries": [],
                        "success": False,
                        "attempts": attempts_used,
                        "error": result.error or "unknown error",
                    }
                )

        print(f"   åˆ†å—ç»“æœ: {success} æˆåŠŸ, {skipped} è·³è¿‡, {chunk_count - success - skipped} å¤±è´¥")
        if any_failed:
            print("âŒ å­˜åœ¨å¤±è´¥ chunkï¼Œå·²è·³è¿‡æœ€ç»ˆåˆå¹¶ã€‚è¯·åŸºäº jsonl è¿›è¡Œé‡è¯•åå†åˆå¹¶ã€‚")
            return 2
        print(f"âœ… å¼€å§‹åˆå¹¶: {len(chunk_results)} chunks")
        entries = extractor.merge_results(chunk_results)
        print(f"âœ… åˆå¹¶å®Œæˆ: {len(entries)} æ¡ç›®")
    
    print(f"âœ… æŠ½å–å®Œæˆ: {len(entries)} ä¸ªæ¡ç›®")
    
    # è¾“å‡ºç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.write_text(
            json.dumps(entries, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"   å·²ä¿å­˜åˆ°: {output_path}")
    else:
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + "=" * 50)
        print(json.dumps(entries, ensure_ascii=False, indent=2))
    
    # å¯¼å…¥åˆ°æ•°æ®åº“
    if args.import_db:
        from core.storage import SQLiteContentStore
        from core.config import get_config
        
        config = get_config()
        store = SQLiteContentStore(config.database.content_path)
        
        for entry in entries:
            # ä½¿ç”¨ name ä½œä¸ºæ•°æ®åº“ IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨åºå·å…œåº•
            entry_id = entry.get("name") or f"wi_{input_path.stem}_{entries.index(entry)}"
            store.save("world_info", entry_id, entry, tags=["extracted"])
        
        print(f"   å·²å¯¼å…¥åˆ° content.db: {len(entries)} ä¸ªæ¡ç›®")
    
    return 0


def cmd_list(args):
    """åˆ—å‡ºå¯ç”¨çš„æŠ½å–å™¨"""
    print("å¯ç”¨çš„æŠ½å–å™¨:")
    print("  worldinfo  - ä¸–ç•Œä¹¦æ¡ç›®æŠ½å–ï¼ˆä»å°è¯´/è®¾å®šæ–‡æœ¬ä¸­æå–ä¸–ç•Œè§‚ï¼‰")
    print()
    print("ç”¨æ³•ç¤ºä¾‹:")
    print("  python -m extraction.run worldinfo input.txt -o output.json")
    return 0


def main():
    from extraction.config import get_extraction_config
    config = get_extraction_config()

    parser = argparse.ArgumentParser(
        description="LLM æ•°æ®æŠ½å–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # list å‘½ä»¤
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºå¯ç”¨çš„æŠ½å–å™¨")
    list_parser.set_defaults(func=cmd_list)
    
    # worldinfo å‘½ä»¤
    wi_parser = subparsers.add_parser("worldinfo", help="ä¸–ç•Œä¹¦æ¡ç›®æŠ½å–")
    wi_parser.add_argument(
        "input",
        nargs="?",
        default=config.input,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨ --input-dir æ—¶å¯çœç•¥ï¼‰",
    )
    wi_parser.add_argument(
        "--input-dir",
        default=config.input_dir,
        help="è¾“å…¥ç›®å½•ï¼ˆè¯»å–ç›®å½•ä¸‹æ‰€æœ‰ .md æ–‡ä»¶ï¼‰",
    )
    wi_parser.add_argument(
        "--no-recursive",
        action="store_false",
        dest="recursive",
        help="ä¸é€’å½’è¯»å–å­ç›®å½•ï¼ˆé»˜è®¤é€’å½’ï¼‰"
    )
    wi_parser.set_defaults(recursive=config.recursive)
    wi_parser.add_argument("-o", "--output", default=config.output, help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    wi_parser.add_argument("--model", default=config.model, help="LLM æ¨¡å‹åç§°")
    wi_parser.add_argument("--temperature", type=float, default=config.temperature, help="ç”Ÿæˆæ¸©åº¦")
    wi_parser.add_argument(
        "--chunk-size",
        type=int,
        default=config.chunk_size,
        help="åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œé»˜è®¤ 8000",
    )
    wi_parser.add_argument(
        "--chunk-strategy",
        choices=["auto", "fixed", "chapters"],
        default=config.chunk_strategy,
        help="åˆ†å—ç­–ç•¥ï¼šauto(æœ‰ç« èŠ‚åˆ™æŒ‰ç« èŠ‚ï¼Œå¦åˆ™å›ºå®š)ã€fixed(å›ºå®šé•¿åº¦)ã€chapters(æŒ‰ç« èŠ‚ï¼Œè¶…é•¿ç« èŠ‚å†åˆ‡)",
    )
    wi_parser.add_argument(
        "--chapter-max",
        type=int,
        default=config.chapter_max,
        help="ç« èŠ‚å—æœ€å¤§å­—ç¬¦æ•°ï¼ˆç« èŠ‚ç­–ç•¥ä¸‹ç”Ÿæ•ˆï¼‰",
    )
    wi_parser.add_argument(
        "--retry-max",
        type=int,
        default=config.retry_max,
        help="chunk æŠ½å–å¤±è´¥çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 3",
    )
    wi_parser.add_argument(
        "--prompts-dir",
        dest="prompts_dir",
        default=config.prompts_dir,
        help="æç¤ºè¯ç›®å½•æˆ–å¥—ä»¶åï¼ˆå¦‚ shi_jiaoï¼‰ï¼Œé»˜è®¤ä½¿ç”¨ extraction/prompts/xiuxian/",
    )
    wi_parser.add_argument(
        "--overlap",
        type=int,
        default=config.overlap,
        help="åˆ†å—é‡å å¤§å°",
    )
    wi_parser.add_argument(
        "--no-llm-merge",
        action="store_false",
        dest="llm_merge",
        help="ç¦ç”¨è·¨ chunk çš„ LLM åˆå¹¶/æ¶ˆæ­§ï¼ˆé»˜è®¤å¯ç”¨ï¼‰"
    )
    wi_parser.set_defaults(llm_merge=config.llm_merge)
    wi_parser.add_argument(
        "--estimate-tokens",
        action="store_true",
        default=config.estimate_tokens,
        help="ä¼°ç®—æç¤ºè¯ tokenï¼ˆä¸åŒ…å«æ¨¡å‹è¾“å‡ºï¼‰",
    )
    wi_parser.add_argument(
        "--estimate-only",
        action="store_true",
        default=config.estimate_only,
        help="åªä¼°ç®— tokenï¼Œä¸æ‰§è¡ŒæŠ½å–",
    )
    wi_parser.add_argument(
        "--output-jsonl",
        default=config.output_jsonl,
        help="å¢é‡å†™å…¥ JSONLï¼ˆæ¯ä¸ªæ–‡ä»¶ä¸€è¡Œï¼Œé˜²æ­¢è¿›åº¦ä¸¢å¤±ï¼‰",
    )
    wi_parser.add_argument(
        "--resume",
        action="store_true",
        default=config.resume,
        help="ä» JSONL æ–­ç‚¹ç»­è·‘ï¼ˆè·³è¿‡å·²å¤„ç†æ–‡ä»¶ï¼‰",
    )
    wi_parser.add_argument(
        "--import-db",
        action="store_true",
        default=config.import_db,
        help="ç›´æ¥å¯¼å…¥åˆ° content.db",
    )
    wi_parser.set_defaults(func=cmd_worldinfo)
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 0
    
    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
