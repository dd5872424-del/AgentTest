"""
æŠ½å–æ¨¡å— CLI å…¥å£

ç”¨æ³•:
    cd backend
    
    # ä»æ–‡ä»¶æŠ½å–ä¸–ç•Œä¹¦
    python -m extraction.run worldinfo input.txt -o output.json
    
    # ä»æ–‡ä»¶æŠ½å–ï¼ˆé•¿æ–‡æœ¬è‡ªåŠ¨åˆ†å—ï¼‰
    python -m extraction.run worldinfo novel.txt -o worldinfo.json --chunk-size 6000
    
    # æŒ‡å®šæ¨¡å‹
    python -m extraction.run worldinfo input.txt --model gpt-4o
    
    # ç›´æ¥å¯¼å…¥åˆ° content.db
    python -m extraction.run worldinfo input.txt --import-db
"""
import argparse
import json
import sys
from pathlib import Path

# ç¡®ä¿ backend ç›®å½•åœ¨ Python è·¯å¾„ä¸­
backend_dir = Path(__file__).parent.parent
if str(backend_dir) not in sys.path:
    sys.path.insert(0, str(backend_dir))


def cmd_worldinfo(args):
    """ä¸–ç•Œä¹¦æŠ½å–å‘½ä»¤"""
    from extraction import WorldInfoExtractor
    from extraction.base import ExtractionResult
    
    print(f"ğŸ“– ä¸–ç•Œä¹¦æŠ½å–")
    print(f"   è¾“å…¥: {args.input}")
    
    # åˆ›å»ºæŠ½å–å™¨
    extractor_kwargs = {}
    if args.model:
        extractor_kwargs["model"] = args.model
    if args.temperature:
        extractor_kwargs["temperature"] = args.temperature
    
    llm_merge_enabled = bool(args.llm_merge)
    preserve_order_enabled = bool(args.preserve_order) or llm_merge_enabled
    extractor = WorldInfoExtractor(
        enable_llm_merge=llm_merge_enabled,
        preserve_order=preserve_order_enabled,
        **extractor_kwargs
    )
    
    # è¯»å–æ–‡ä»¶æˆ–ç›®å½•
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨: {input_path}")
        return 1

    # ç›®å½•æ¨¡å¼ï¼šè¯»å–è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰ .mdï¼ˆæŒ‰æ–‡ä»¶åé¡ºåºï¼‰
    if input_path.is_dir():
        md_files = sorted(input_path.glob("*.md"))
        if not md_files:
            print(f"âŒ ç›®å½•å†…æœªæ‰¾åˆ° .md æ–‡ä»¶: {input_path}")
            return 1
        print(f"   æ–‡ä»¶æ•°: {len(md_files)}")

        results_all = []
        for md_path in md_files:
            text = md_path.read_text(encoding="utf-8")
            print(f"   å¤„ç†: {md_path.name} ({len(text)} å­—ç¬¦)")

            if args.chunk_size and len(text) > args.chunk_size:
                print(f"   åˆ†å—å¤„ç†: æ¯å— {args.chunk_size} å­—ç¬¦")
                results = extractor.extract_chunks(
                    text,
                    chunk_size=args.chunk_size,
                    overlap=args.overlap or 500
                )

                # ç›®å½•æ¨¡å¼ä¸‹é¿å…å¯¹æ¯ä¸ªæ–‡ä»¶å•ç‹¬åš LLM åˆå¹¶ï¼Œç•™ç»™å…¨å±€åˆå¹¶å¤„ç†
                if args.llm_merge:
                    prev_merge = extractor.enable_llm_merge
                    extractor.enable_llm_merge = False
                    entries = extractor.merge_results(results)
                    extractor.enable_llm_merge = prev_merge
                else:
                    entries = extractor.merge_results(results)

                success_count = sum(1 for r in results if r.success)
                print(f"   åˆ†å—ç»“æœ: {success_count}/{len(results)} æˆåŠŸ")
            else:
                print(f"   æŠ½å–ä¸­...")
                result = extractor.extract(text)
                if not result.success:
                    print(f"âŒ æŠ½å–å¤±è´¥: {result.error}")
                    return 1
                entries = result.data

            results_all.append(ExtractionResult(
                success=True,
                data=entries,
                source=str(md_path),
            ))

        # ç›®å½•æ¨¡å¼åˆå¹¶æ‰€æœ‰æ–‡ä»¶ï¼ˆæŒ‰æ–‡ä»¶é¡ºåºï¼‰
        entries = extractor.merge_results(results_all)
    else:
        # æ–‡ä»¶æ¨¡å¼
        text = input_path.read_text(encoding="utf-8")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")

        # æ‰§è¡ŒæŠ½å–
        if args.chunk_size and len(text) > args.chunk_size:
            print(f"   åˆ†å—å¤„ç†: æ¯å— {args.chunk_size} å­—ç¬¦")
            results = extractor.extract_chunks(
                text,
                chunk_size=args.chunk_size,
                overlap=args.overlap or 500
            )
            entries = extractor.merge_results(results)

            # ç»Ÿè®¡æˆåŠŸ/å¤±è´¥
            success_count = sum(1 for r in results if r.success)
            print(f"   åˆ†å—ç»“æœ: {success_count}/{len(results)} æˆåŠŸ")
        else:
            print(f"   æŠ½å–ä¸­...")
            result = extractor.extract(text)

            if not result.success:
                print(f"âŒ æŠ½å–å¤±è´¥: {result.error}")
                return 1

            entries = result.data
    
    print(f"âœ… æŠ½å–å®Œæˆ: {len(entries)} ä¸ªæ¡ç›®")
    
    # è¾“å‡ºç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.write_text(
            json.dumps(entries, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"   å·²ä¿å­˜åˆ°: {output_path}")
    else:
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + "=" * 50)
        print(json.dumps(entries, ensure_ascii=False, indent=2))
    
    # å¯¼å…¥åˆ°æ•°æ®åº“
    if args.import_db:
        from core.storage import SQLiteContentStore
        from core.config import get_config
        
        config = get_config()
        store = SQLiteContentStore(config.database.content_path)
        
        for entry in entries:
            # ä½¿ç”¨ name ä½œä¸ºæ•°æ®åº“ IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨åºå·å…œåº•
            entry_id = entry.get("name") or f"wi_{input_path.stem}_{entries.index(entry)}"
            store.save("world_info", entry_id, entry, tags=["extracted"])
        
        print(f"   å·²å¯¼å…¥åˆ° content.db: {len(entries)} ä¸ªæ¡ç›®")
    
    return 0


def cmd_list(args):
    """åˆ—å‡ºå¯ç”¨çš„æŠ½å–å™¨"""
    print("å¯ç”¨çš„æŠ½å–å™¨:")
    print("  worldinfo  - ä¸–ç•Œä¹¦æ¡ç›®æŠ½å–ï¼ˆä»å°è¯´/è®¾å®šæ–‡æœ¬ä¸­æå–ä¸–ç•Œè§‚ï¼‰")
    print()
    print("ç”¨æ³•ç¤ºä¾‹:")
    print("  python -m extraction.run worldinfo input.txt -o output.json")
    return 0


def main():
    parser = argparse.ArgumentParser(
        description="LLM æ•°æ®æŠ½å–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # list å‘½ä»¤
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºå¯ç”¨çš„æŠ½å–å™¨")
    list_parser.set_defaults(func=cmd_list)
    
    # worldinfo å‘½ä»¤
    wi_parser = subparsers.add_parser("worldinfo", help="ä¸–ç•Œä¹¦æ¡ç›®æŠ½å–")
    wi_parser.add_argument("input", help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    wi_parser.add_argument("-o", "--output", help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„")
    wi_parser.add_argument("--model", help="LLM æ¨¡å‹åç§°")
    wi_parser.add_argument("--temperature", type=float, help="ç”Ÿæˆæ¸©åº¦")
    wi_parser.add_argument("--chunk-size", type=int, help="åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰ï¼Œç”¨äºå¤„ç†é•¿æ–‡æœ¬")
    wi_parser.add_argument("--overlap", type=int, default=500, help="åˆ†å—é‡å å¤§å°")
    wi_parser.add_argument("--llm-merge", action="store_true", help="å¯ç”¨è·¨ chunk çš„ LLM åˆå¹¶/æ¶ˆæ­§ï¼ˆä¼šé¢å¤–è°ƒç”¨ä¸€æ¬¡ LLMï¼‰")
    wi_parser.add_argument("--preserve-order", action="store_true", help="ä¿ç•™æ¡ç›®é¡ºåºï¼ˆä¸æŒ‰ä¼˜å…ˆçº§æ’åºï¼‰")
    wi_parser.add_argument("--import-db", action="store_true", help="ç›´æ¥å¯¼å…¥åˆ° content.db")
    wi_parser.set_defaults(func=cmd_worldinfo)
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 0
    
    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
